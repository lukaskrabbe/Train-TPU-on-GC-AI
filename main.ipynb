{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification from scratch with TPUs on Cloud ML Engine using ResNet\n",
    "\n",
    "This notebook demonstrates how to do image classification from scratch on a flowers dataset using TPUs and the resnet trainer.\n",
    "\n",
    "A detailed Tutorial can be find under the following Link:\n",
    "\n",
    "https://medium.com/p/c71b6eed78e0/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Project Sttings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next Step we have to set some important Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GC_PROJECT =''             # REPLACE WITH YOUR GOOGLE PROJECT ID\n",
    "BUCKET = 'product-classification'         # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1'                    # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "PROJECT = 'project_001_freisteller'       # REPLACE WITH YOUR <YOUR_PROJECT_NAME>\n",
    "MODEL_VERSION = 'ResNet_v04'              # REPLACE WITH A MODEL NAME eg. ResNet_v01\n",
    "\n",
    "\n",
    "# do not change these\n",
    "os.environ['GC_PROJECT'] = GC_PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION\n",
    "os.environ['TFVERSION'] = '1.14'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $GC_PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mounting Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Notebook will need Access to the created Bucket in the GCS\n",
    "\n",
    "this will be achived by mounting the bucket to the the Folder /home/jupyter/<YOUR_PROJECT_NAME> (the top Folder of the notebook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fusermount -u /home/jupyter/$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/gcsfuse $BUCKET /home/jupyter/$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Enable TPU service account\n",
    "\n",
    "Allow Cloud ML Engine to access the TPU and bill to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SVC_ACCOUNT=$(curl -H \"Authorization: Bearer $(gcloud auth print-access-token)\"  \\\n",
    "    https://ml.googleapis.com/v1/projects/${GC_PROJECT}:getConfig \\\n",
    "              | grep tpuServiceAccount | tr '\"' ' ' | awk '{print $3}' )\n",
    "              \n",
    "echo \"Enabling TPU service account $SVC_ACCOUNT to act as Cloud ML Service Agent\"\n",
    "gcloud projects add-iam-policy-binding $GC_PROJECT --member serviceAccount:$SVC_ACCOUNT --role roles/ml.serviceAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Local Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need an empty Folder for our local Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ${PWD}/local_test/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our local test we copy the first 5 Lines to a file in our local Test Folder\n",
    "\n",
    "Furthermore we will need the labels.txt file from the GC Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cat gs://${BUCKET}/${PROJECT}/train_set.csv | head -5 > ${PWD}/local_test/input.csv\n",
    "gsutil cat gs://${BUCKET}/${PROJECT}/labels.txt > ${PWD}/local_test/labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we will start the local preprocessing, the code for this task is placed in the mymodel/trainer/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/mymodel\n",
    "\n",
    "python -m trainer.preprocess \\\n",
    "       --train_csv ${PWD}/local_test/input.csv \\\n",
    "       --validation_csv ${PWD}/local_test/input.csv \\\n",
    "       --labels_file ${PWD}/local_test/labels.txt \\\n",
    "       --project_id $PROJECT \\\n",
    "       --output_dir ${PWD}/local_test/out \\\n",
    "       --runner=DirectRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ${PWD}/local_test/out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Online Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we successfully created the small local Datasets, we will preprocess all our Data.\n",
    "\n",
    "To speed this step up, we will run the Preprocessing with Googles dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/mymodel\n",
    "    \n",
    "gsutil -m rm -rf gs://${BUCKET}/resnet/data\n",
    "python -m trainer.preprocess \\\n",
    "       --train_csv gs://${BUCKET}/${PROJECT}/train_set.csv \\\n",
    "       --validation_csv gs://${BUCKET}/${PROJECT}/eval_set.csv \\\n",
    "       --labels_file gs://${BUCKET}/${PROJECT}/labels.txt \\\n",
    "       --project_id $GC_PROJECT \\\n",
    "       --output_dir gs://${BUCKET}/${PROJECT}/resnet/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the progress under the follwoing link:\n",
    "\n",
    "https://console.cloud.google.com/dataflow\n",
    "\n",
    "After the Preprocessing is done, you will see the created Train and Validataion Datasets with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/${PROJECT}/resnet/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Train on the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -n \"--num_train_images=$(gsutil cat gs://${BUCKET}/${PROJECT}/train_set.csv | wc -l)  \"\n",
    "echo -n \"--num_eval_images=$(gsutil cat gs://${BUCKET}/${PROJECT}/eval_set.csv | wc -l)  \"\n",
    "echo \"--num_label_classes=$(gsutil cat gs://${BUCKET}/${PROJECT}/labels.txt | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the previous Line into the follwoing Block (watch the Comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOPDIR=gs://${BUCKET}/${PROJECT}/resnet\n",
    "OUTDIR=${TOPDIR}/trained\n",
    "JOBNAME=imgclass_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR  # Comment out this line to continue training from the last time\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.resnet_main \\\n",
    " --package-path=$(pwd)/mymodel/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --staging-bucket=gs://$BUCKET \\\n",
    " --scale-tier=BASIC_TPU \\\n",
    " --runtime-version=$TFVERSION --python-version=3.5 \\\n",
    " -- \\\n",
    " --data_dir=${TOPDIR}/data \\\n",
    " --model_dir=${OUTDIR}/ \\\n",
    " --resnet_depth=18 \\\n",
    " --train_batch_size=128 --eval_batch_size=64 --skip_host_call=True \\\n",
    " --steps_per_eval=150 --train_steps=500 \\\n",
    " --num_train_images=865 --num_eval_images=94 --num_label_classes=6 \\      # Modify here !\n",
    " --export_dir=${OUTDIR}/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this on your Cloud Console:\n",
    "\n",
    "tensorboard --logdir=gs://product-classification/project_001_freisteller/resnet/trained/ --port=8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Deploying and predicting with model\n",
    "\n",
    "To allow Classification in the production Mode, we have to deploy out trained model to the AI-Platform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Choose your Option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you want to create a complete new Model\n",
    "\n",
    "# gcloud ml-engine models create ${PROJECT} --regions $REGION\n",
    "# gcloud ml-engine versions create ${MODEL_VERSION} --model ${PROJECT} --origin $(gsutil ls gs://${BUCKET}/${PROJECT}/resnet/trained/export/ | tail -1) --runtime-version=${TFVERSION} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you want to deploy an new Modell Version\n",
    "\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model=${PROJECT} --origin=$(gsutil ls gs://${BUCKET}/${PROJECT}/resnet/trained/export/ | tail -1) --runtime-version=${TFVERSION} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you want to create delet an Verion or an Modell\n",
    "\n",
    "# %%bash\n",
    "# gcloud ml-engine versions delete --quiet ${MODEL_VERSION} --model ${PROJECT}\n",
    "# gcloud ml-engine models delete ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Test Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Functions are designed to do a quick test if your Model works:\n",
    "\n",
    "(You can choose between the pictures you already downloaded into the Google Cloud Storage\n",
    "and picture which are available in the Web on a given URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import base64, sys, json\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import io\n",
    "import requests\n",
    "\n",
    "\n",
    "def plot_and_predict_from_GS(picture):\n",
    "    with open('/home/jupyter/' + PROJECT + '/local_test/labels.txt', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    \n",
    "    with tf.gfile.GFile('gs://' + BUCKET + '/' + PROJECT + '/images/'+ picture, 'rb') as ifp:\n",
    "        credentials = GoogleCredentials.get_application_default()\n",
    "        api = discovery.build('ml', 'v1', credentials=credentials, discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "        \n",
    "        request_data = {'instances':\n",
    "                        [\n",
    "                            {\"input\": {\"b64\": base64.b64encode(ifp.read()).decode('utf-8')}}\n",
    "                        ]}\n",
    "        parent = 'projects/%s/models/%s/versions/%s' % (GC_PROJECT, PROJECT, MODEL_VERSION)\n",
    "        response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "\n",
    "\n",
    "        img=mpimg.imread('/home/jupyter/' + BUCKET + '/' + PROJECT + '/images/' + picture)\n",
    "        imgplot = plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "        print(lines[response['predictions'][0]['probabilities'].index(max(response['predictions'][0]['probabilities']))])\n",
    "\n",
    "def plot_and_predict_from_Local(path):\n",
    "    with open('/home/jupyter/' + PROJECT + '/local_test/labels.txt', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    with open(path, 'rb') as ifp:\n",
    "        credentials = GoogleCredentials.get_application_default()\n",
    "        api = discovery.build('ml', 'v1', credentials=credentials, discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "        request_data = {'instances':\n",
    "                        [\n",
    "                            {\"input\": {\"b64\": base64.b64encode(ifp.read()).decode('utf-8')}}\n",
    "                        ]}\n",
    "        parent = 'projects/%s/models/%s/versions/%s' % (GC_PROJECT, PROJECT, MODEL_VERSION)\n",
    "        response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "\n",
    "        \n",
    "        img=mpimg.imread(path)\n",
    "        imgplot = plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "        print(lines[response['predictions'][0]['probabilities'].index(max(response['predictions'][0]['probabilities']))])\n",
    "        print(response['predictions'][0]['probabilities'].index(max(response['predictions'][0]['probabilities'])))\n",
    "        \n",
    "def plot_and_predict_from_URL(url):\n",
    "    with open('/home/jupyter/' + PROJECT + '/local_test/labels.txt', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    response = requests.get(url)\n",
    "\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    api = discovery.build('ml', 'v1', credentials=credentials, discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "    \n",
    "    request_data = {'instances':\n",
    "                    [\n",
    "                        {\"input\": {\"b64\": base64.b64encode(response.content).decode('utf-8')}}\n",
    "                    ]}\n",
    "    parent = 'projects/%s/models/%s/versions/%s' % (GC_PROJECT, PROJECT, MODEL_VERSION)\n",
    "    response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "\n",
    "\n",
    "        \n",
    "    image = io.imread(url)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #print(response)\n",
    "    print(lines[response['predictions'][0]['probabilities'].index(max(response['predictions'][0]['probabilities']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pictures = ['12812786.jpg', '13369544.jpg', '15452981.jpg', '19103367.jpg', '20087011.jpg']\n",
    "\n",
    "for picture in pictures:\n",
    "    plot_and_predict_from_GS(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tchibo_0 = 'https://www.tchibo.de/newmedia/art_img/MAIN-CENSHARE/82e052bdd86c785b/metall-schubladenturm.jpg'\n",
    "url_tchibo_1 = 'https://www.tchibo.de/newmedia/art_img/MAIN-CENSHARE/aaec39febb0dc1c7/max-winzer-federkern-eckschlafsofa-mit-stauraumbank.jpg'\n",
    "url_cnouch = 'https://i.cnouch.de/i/otto/28333726/Jockenhoefer-Gruppe-Schlafsofa-inkl-Bettkasten-28333726.jpg?maxW=998&maxH=562'\n",
    "\n",
    "\n",
    "plot_and_predict_from_URL(url_tchibo_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
