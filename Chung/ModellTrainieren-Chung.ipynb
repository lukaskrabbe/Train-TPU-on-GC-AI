{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import wget\n",
    "import os\n",
    "import shutil\n",
    "from google.cloud import storage\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context('notebook', font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "from sklearn.utils import shuffle\n",
    "import umap\n",
    "\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "BUCKET_NAME = 'product-classification'\n",
    "PROJECT_NAME = 'project_001_freisteller'\n",
    "path = 'gs://' + BUCKET_NAME + '/' + PROJECT_NAME + '/'\n",
    "IMAGE_PATH = 'images'\n",
    "MOUNTED_PATH = '/home/jupyter/product-classification/'\n",
    "CORE_FILE = 'OTTO_MAK_Bilddaten.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core_df = pd.read_excel(path + CORE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/product-classification/project_001_freisteller/images/\n",
      "959\n"
     ]
    }
   ],
   "source": [
    "print((MOUNTED_PATH+PROJECT_NAME+'/'+IMAGE_PATH+'/'))\n",
    "print(len([name for name in os.listdir((MOUNTED_PATH+PROJECT_NAME+'/'+IMAGE_PATH+'/'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir /home/jupyter/product-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/gcsfuse product-classification /home/jupyter/product-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/jupyter/product-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hier fängt das Trainieren des InceptionResNetV2-Modells an*\n",
    "\n",
    "Beschreibung einzelner Prozessschritte des Gesamtprozesses:\n",
    "1. Basismodell von InceptionResNetV2 anwenden. Wir verwenden die vortrainierten Gewichte anhand Imagenet, werden später eigene Output-Schichten (z.B MaxPooling layer, Dense layer mit bestimmter Anzahl von Klassen) einbauen.\n",
    "=> include_top= False\n",
    "2. ImageDataGenerator werden verwendet, um Pixeln von Bildern zu kodieren.\n",
    "3. wir definieren Trainingsdaten, Validierungsdaten und Testdaten\n",
    "4. Wir verwenden flow_from_dataframe, um Bilder aus dem Ordner zu lesen und sie jeweils zu ihrer Klasse zuzuordnen. Dafür ist eine Tabelle mit zwei Spalten (filenames und classes) nötig. \n",
    "5. Für jede Tabelle (Trainings-,Validierung-,Testdaten) flow_from_dataframe anwenden. Bei Trainingsdaten und Validierungsdaten: Shuffel: True, y_col: Klasse, Bei Testdaten: Shuffel: false, y_col: None, default Batch_Size= 1\n",
    "6. Batch_Size so definieren, dass sie nicht größer als die Datenmenge ist.\n",
    "7. Basismodell mit zusätzlichen Schichten einbauen. Somit entsteht das endgültige Modell.\n",
    "8. Mit fit_generator das Modell mit den Trainingsdaten trainieren.\n",
    "9. Mit predict_generator das Modell mit den Testdaten testen. \n",
    "10. Mit predicted_class_indices bekommen wir am Ende vom Modell vorhergesagte Klassen für jeweilige Datensätze in den Testdaten. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare model to encode images\n",
    "\n",
    "incesnet = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "\n",
    "generator = ImageDataGenerator(rescale= 1. / 255)\n",
    "generator2= ImageDataGenerator(rescale= 1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = pd.read_csv(path + 'train_classifier.csv')\n",
    "classified_df['Label'] = classified_df['Label'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {0: 'Zweifarbig',\n",
    "               1: 'Einfarbig',\n",
    "               2: 'Freisteller',\n",
    "               3: 'Ambiente',\n",
    "               4: 'Abmaßungen',\n",
    "               5: 'Sonstige'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 35\n",
    "# set the path to the serialized model after training\n",
    "MODEL_PATH = os.path.sep.join([\"output\", \"food11.model\"])\n",
    " \n",
    "# define the path to the output training history plots\n",
    "UNFROZEN_PLOT_PATH = os.path.sep.join([\"output\", \"unfrozen.png\"])\n",
    "WARMUP_PLOT_PATH = os.path.sep.join([\"output\", \"warmup.png\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingsdata =classified_df[:100]\n",
    "#validationdata=classified_df[100:200]\n",
    "#testdata=classified_df[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(H, N, plotPath):\n",
    "    # construct a plot that plots and saves the training history\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingsdata = classified_df.sample(n=300)\n",
    "validationdata = classified_df.sample(n=50)\n",
    "testdata = classified_df.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label  Count\n",
      "0     5     30\n",
      "1     2     91\n",
      "2     3     69\n",
      "3     1     38\n",
      "4     4     48\n",
      "5     0     24\n"
     ]
    }
   ],
   "source": [
    "trainingsdata_overview = trainingsdata.groupby('Label', sort=False).size().reset_index(name='Count')\n",
    "print (trainingsdata_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gen = generator.flow_from_dataframe(dataframe=trainingsdata,\n",
    "                                    shuffle=True,\n",
    "                                    directory=(MOUNTED_PATH + PROJECT_NAME + '/' + IMAGE_PATH),\n",
    "                                    x_col='Image',\n",
    "                                    y_col='Label',\n",
    "                                    target_size=(299, 299),\n",
    "                                    batch_size=BATCH_SIZE,  \n",
    "                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = generator2.flow_from_dataframe(dataframe=validationdata,\n",
    "                                    shuffle=True,\n",
    "                                    directory=(MOUNTED_PATH + PROJECT_NAME + '/' + IMAGE_PATH),\n",
    "                                    x_col='Image',\n",
    "                                    y_col='Label',\n",
    "                                    target_size=(299, 299),\n",
    "                                    batch_size=BATCH_SIZE,  \n",
    "                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator2.flow_from_dataframe(dataframe=testdata,\n",
    "                                    shuffle=False,\n",
    "                                    directory=(MOUNTED_PATH + PROJECT_NAME + '/' + IMAGE_PATH),\n",
    "                                    x_col='Image',\n",
    "                                    y_col=None ,\n",
    "                                    target_size=(299, 299),\n",
    "                                    batch_size=1,  \n",
    "                                    class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Activation, PReLU\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "\n",
    "x = incesnet.output\n",
    "x = Flatten(name=\"flatten\")(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(label_names), activation='softmax')(x) \n",
    "model = Model(inputs=incesnet.input, outputs=predictions)\n",
    "# Freezing Convolutional layers\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in incesnet.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print(model.summary())\n",
    "        \n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=training_gen.n//training_gen.batch_size\n",
    "STEP_SIZE_VALID=valid_gen.n//valid_gen.batch_size\n",
    "STEP_SIZE_TEST=test_gen.n//test_gen.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit_generator(generator=training_gen,\n",
    "                     steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "                      validation_data=valid_gen, #valid_gen\n",
    "                       validation_steps=STEP_SIZE_VALID,\n",
    "                         epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen.reset()\n",
    "pred=model.predict_generator(test_gen,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#from pyimagesearch import config\n",
    "\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "labels = (training_gen.class_indices)\n",
    "labels2 = dict((v,k) for k,v in label_names.items())\n",
    "predictions = [label_names[k] for k in predicted_class_indices]\n",
    "print(predicted_class_indices)\n",
    "print (labels)\n",
    "print (predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = [k for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Zweifarbig',\n",
    "                'Einfarbig',\n",
    "                'Freisteller',\n",
    "                'Ambiente',\n",
    "                'Abmaßungen',\n",
    "                'Sonstige']\n",
    "\n",
    "print(classification_report(testdata['Label'].astype(str).astype(int),predictions2, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(hist):\n",
    "    \"\"\"\n",
    "    Plots the plots from a training history.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : history of a model fit (required)\n",
    "    \"\"\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset our data generators\n",
    "training_gen.reset()\n",
    "valid_gen.reset()\n",
    " \n",
    "# now that the head FC layers have been trained/initialized, lets\n",
    "# unfreeze the final set of CONV layers and make them trainable\n",
    "for layer in incesnet.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in incesnet.layers:\n",
    "    print(\"{}: {}\".format(layer, layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# for the changes to the model to take affect we need to recompile\n",
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "\n",
    "print(\"[INFO] re-compiling model...\")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer = opt, \n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = model.fit_generator(\n",
    "    training_gen,\n",
    "    steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "    validation_data=valid_gen, #valid_gen\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "test_gen.reset()\n",
    "pred = model.predict_generator(test_gen,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "labels = (training_gen.class_indices)\n",
    "labels2 = dict((v,k) for k,v in label_names.items())\n",
    "predictions = [label_names[k] for k in predicted_class_indices]\n",
    "print(predicted_class_indices)\n",
    "print (labels)\n",
    "print (predictions)\n",
    "\n",
    "predictions2 = [k for k in predicted_class_indices]\n",
    "\n",
    "target_names = ['Zweifarbig',\n",
    "                'Einfarbig',\n",
    "                'Freisteller',\n",
    "                'Ambiente',\n",
    "                'Abmaßungen',\n",
    "                'Sonstige']\n",
    "\n",
    "print(classification_report(testdata['Label'].astype(str).astype(int),predictions2, target_names = target_names))\n",
    "\n",
    "\n",
    "\n",
    "#plot_training(H, 10, UNFROZEN_PLOT_PATH)\n",
    " \n",
    "# serialize the model to disk\n",
    "#print(\"[INFO] serializing network...\")\n",
    "#model.save(config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## test sample pictures ############\n",
    "\n",
    "testdata2 = classified_df.sample(n=1)\n",
    "\n",
    "print(testdata2)\n",
    "\n",
    "sample_gen = generator2.flow_from_dataframe(dataframe=testdata2,\n",
    "                                    shuffle=False,\n",
    "                                    directory=(MOUNTED_PATH + PROJECT_NAME + '/' + IMAGE_PATH),\n",
    "                                    x_col='Image',\n",
    "                                    y_col=None ,\n",
    "                                    target_size=(299, 299),\n",
    "                                    batch_size=1,  \n",
    "                                    class_mode=None)\n",
    "\n",
    "#sample_gen.reset()\n",
    "pred = model.predict_generator(sample_gen,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "labels = (training_gen.class_indices)\n",
    "predictions = [label_names[k] for k in predicted_class_indices]\n",
    "\n",
    "\n",
    "print(\"prediction: \")\n",
    "print(predicted_class_indices)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = Image.open('/home/jupyter/product-classification/project_001_freisteller/images/20192535.jpg') #jpg\n",
    "image.show()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save result\n",
    "\n",
    "#imagefile=test_gen.filenames\n",
    "#results=pd.DataFrame({\"Image\":imagefile,\n",
    "#                      \"Prediction\":predictions})\n",
    "#results.to_csv(\"results_test1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import datetime\n",
    "\n",
    "# Export the model to a file\n",
    "filename = 'model.joblib'\n",
    "\n",
    "joblib.dump(model, filename)\n",
    " \n",
    "\n",
    "\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(BUCKET_NAME)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    datetime.datetime.now().strftime('census_%Y%m%d_%H%M%S'),\n",
    "    model))\n",
    "blob.upload_from_filename(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis Hier :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "BUCKET_NAME = 'product-classification'\n",
    "PROJECT_NAME = 'project_001_freisteller'\n",
    "\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "public_bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = public_bucket.blob('project_001_freisteller/classified.csv')\n",
    "blob.download_to_filename('classified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a timestamped job name\n",
    "JOB_NAME = \"census_training_{}\".format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ./__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the training job:\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --job-dir gs://$BUCKET_NAME/project_001_freisteller \\\n",
    "  --package-path ./ \\\n",
    "  --module-name project_001_freisteller.train \\\n",
    "  --region us-central1 \\\n",
    "  --runtime-version=1.12 \\\n",
    "  --python-version=3.5 \\\n",
    "  --scale-tier BASIC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
